{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3085c01a-319a-4cd3-acd7-8ad7e4c29869",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "dest_dir = \"/tmp/standard_projections\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "26b04227-ab16-4f50-9dbc-b02d6281dcab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/mojmir/projects/etl/data/meadow/wpp/2019/standard_projections/fertility_by_age.feather',\n",
       " '/Users/mojmir/projects/etl/data/meadow/wpp/2019/standard_projections/location_codes.feather',\n",
       " '/Users/mojmir/projects/etl/data/meadow/wpp/2019/standard_projections/population_by_age_sex.feather',\n",
       " '/Users/mojmir/projects/etl/data/meadow/wpp/2019/standard_projections/total_population.feather',\n",
       " '/Users/mojmir/projects/etl/data/meadow/wpp/2019/standard_projections/variant_codes.feather']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from owid import catalog\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from etl.paths import BASE_DIR as base_path\n",
    "\n",
    "d = catalog.Dataset(\n",
    "    (base_path / \"data/meadow/wpp/2019/standard_projections\").as_posix()\n",
    ")\n",
    "\n",
    "# all tables in dataset\n",
    "d._data_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52818f78",
   "metadata": {},
   "source": [
    "## Harmonize country names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bc276ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from etl.paths import STEP_DIR\n",
    "\n",
    "# TODO: is there a better way? it's hard to obtain notebook's path\n",
    "# maybe add it as a papermill parameter?\n",
    "with open(\n",
    "    STEP_DIR / \"data/garden/wpp/2019/standard_projections_country_mappings.json\"\n",
    ") as f:\n",
    "    country_mappings = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "179fbbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unassigned 233 locations, examples ['Asia-Pacific Economic Cooperation (APEC)', 'African Group', 'World Bank Regional Groups (developing only)', 'Land-locked Countries', 'Shanghai Cooperation Organization (SCO)', 'WB region: Europe and Central Asia (excluding high income)', 'State of Palestine', 'ESCAP: WB income groups', 'WHO: Eastern Mediterranean Region (EMRO)', 'United Nations Economic Commission for Africa (UN-ECA)']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "empty_dataset = catalog.Dataset.create_empty(dest_dir)\n",
    "empty_dataset.metadata = d.metadata\n",
    "\n",
    "for t in d:\n",
    "    # skip these since we already have harmonized country names\n",
    "    if t.metadata.short_name in (\"location_codes\", \"variant_codes\"):\n",
    "        continue\n",
    "\n",
    "    tc = t.reset_index()\n",
    "\n",
    "    harmonized_locations = tc.location.map(country_mappings)\n",
    "    unassigned_locations = set(tc.location[harmonized_locations.isnull()])\n",
    "\n",
    "    # TODO: it would be nice to have ignored locations in `standard_projections_country_mappings.json` so that it\n",
    "    # is clear during review what have we ignored\n",
    "    print(\n",
    "        f\"Unassigned {len(unassigned_locations)} locations, examples {random.sample(list(unassigned_locations), 10)}\"\n",
    "    )\n",
    "\n",
    "    # harmonize\n",
    "    tc = tc[harmonized_locations.notnull()]\n",
    "    tc.location = tc.location.map(country_mappings)\n",
    "\n",
    "    # remove duplicate countries\n",
    "    # TODO: make sure our mapping to countries does not result in two different values\n",
    "    tc = tc.drop_duplicates(subset=t.index.names)\n",
    "\n",
    "    tc = tc.set_index(t.index.names)\n",
    "\n",
    "    # TODO: I would expect `add` to only assign table to dataset, not save it, and then `save` to save everything in dest_dir\n",
    "    # (similarly to commit), but perhaps that isn't memory efficient in case of large datasets?\n",
    "    empty_dataset.add(tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fd571ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/standard_projections'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_dataset.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b71893ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataset to dest_dir\n",
    "# TODO: it is confusing that this doesn't save the data, but only metadata that has been already added through `add`\n",
    "empty_dataset.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
